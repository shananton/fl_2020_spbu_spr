Несложно заметить, что спецификация *не совсем* соответствует коду, который был написан у меня в HW04.
К следующей неделе изменится код, а не спецификация.

Разбор текста программы происходит в два этапа: лексинг и парсинг (или как это называется???).

## Лексер

#### Здесь не все контекстно-свободно...

В этой грамматике терминалам (обычно) соответствуют непосредственно символы входного потока.

В грамматике терминалы -- это все, что будет написано ЗАГЛАВНЫМИ БУКВАМИ, \epsilon, а также все в одинарных кавычках.
Все остальное -- нетерминалы.
Стартовый нетерминал -- stream.

Определим некоторые базовые наборы символов:

```
alpha -> 'a' | 'b' | ... | 'z' | 'A' | ... | 'Z' | '_'
num -> '0' | '1' | ... | '9'
alnum -> alpha | num
hex -> '0' | '1' | ... | '9' | 'a' | ... | 'f' | 'A' | ... | 'F'
wspace -> ' ' | '\t'
```

Перед началом разбора (хотя в коде это может быть одновременно с разбором) производятся преобразования (по порядку):

1. Текст разбивается на строки. Конец строки -- '\n', либо '\r\n', либо конец файла;
    далее эти символы не считаются частью строки.
2. Удаляются все строки вида `wspace*`.
3. Для любой строки, заканчивающейся на `'\' wspace*`, она присоединяется в начало следующей за ней строки (если такая есть).
    `'\' wspace*` удаляется.
4. Для каждой строки определяется величина отступа -- количество пробельных символов до первого непробельного.
    Отступ удаляется из начала строки
5. Затем каждая строка разбивается на токены.
    Уже есть КС-грамматика, но разбор производится не строго по ней.

    Во-первых, есть подсказки вида `[!...]`. Запись `[!{abc}]` означает, что во входном потоке
    в этом месте не могут находиться символы `'a', 'b', 'c'`.
    Запись `[!num]` означает `[!{0123456789}]`, и т. д.
    При этом всегда требуется информация не более, чем об одном следующем символе,
    что позволяет сохранить эффективность разбора.

    Во-вторых, операторы разбираются также не совсем согласно грамматике (подробнее в комментарии).

    ```
    line -> \epsilon | token (wspace* token)*
    token -> nat | op | kword | ident

    # Числа только неотрицательные, но далее мы введем поддержку унарных '+' и '-'.
    # Поддерживаются две системы счисления, допускаются лидирующие нули
    # В токен записывается не последовательность символов, а численное значение.
    nat -> (nat_hex | nat_dec) [!alnum]
    nat_hex -> ('0x' | '0X' ) hex+
    nat_dec -> num+

    # После оператора может находиться любой символ. Для разрешения конфликта используется правило:
    # Операторы разбираются в порядке уменьшения длины (без учета корректности "остатка" строки)
    # Например, '>-' -- это '>' '-'; '>==' -- это '>=' '=' (а не '>' '==');
    # ЕЕсли в язык добавить, например, оператор '=~', то '/=~' не будет разобрано, как '/' '=~',
    # вместо этого, будет найден '/=', а затем разбор завершится неудачей.
    #
    # Скобки для удобства также включены в число операторов.
    op -> '==' | '/=' | '>=' | '<=' | '&&' | '||' | '<' | '>' | '+' | '-' | '*' | '/' | '^' | '=' | '(' | ')' 

    kword -> ('if' | 'then' | 'else' | 'while' | 'do' | 'read' | 'write') [!alnum]

    # Формально <кроме kword> можно выразить,
    # перечислив все короткие идентификаторы отдельно,
    # но на практике это не очень полезный подход.
    ident -> (alpha alnum*) [!alnum] <кроме kword>
    ```

6. Результат разбора строк объединяется с учетом выравнивания.
    Грамматика:

    ```
    stream -> (INDENT | DEDENT*) line NL
    ```

    INDENT, DEDENT, NL -- терминальные символы, но их нет во входном потоке, поэтому стоит объяснить, кто же их туда положил...
    NL просто добавляется в разбор после каждой строки.
    С INDENT и DEDENT все сложнее. Это идея,
    [украденная у питона](https://docs.python.org/3/reference/lexical_analysis.html#indentation).

    Мы не просто так считали отступ у каждой строки. (в питоне это делается лучше, чем просто количество
    пробельных символов, но и так сойдет...).

    Можно считать, что строки разбираются по порядку, при этом есть состояние в виде стека. Изначально на стеке только число 0.

    Дойдя до новой строки, мы смотрим на значение ее отступа `line_indent`. Если оно совпадает с верхним на стеке,
    сразу переходим к разбору строки. Если `line_indent` больше, добавляем его на стек, а в начало строки токен INDENT.
    Если `line_indent` меньше, снимаем со стека значения, пока не наступит РАВЕНСТВО, каждый раз добавляя токен DEDENT.
    (если в стеке нет равного `line_indent` числа, разбор завершается неудачей).

    Дойдя до конца файла, снимаем все числа, кроме первого нуля, каждый раз добавляя DEDENT.

## Парсер

Здесь используется другая грамматика, в которой терминалами являются определенные поддеревья разбора лексера.
В частности, это NAT (бывший nat), IDENT (бывший ident), а также все ключевые слова и операторы.

В грамматике терминалы -- это все, что будет написано ЗАГЛАВНЫМИ БУКВАМИ, а также все в одинарных кавычках.
Все остальное -- нетерминалы.
Стартовый нетерминал -- program.

Грамматика (на этот раз, действительно контекстно свободная):

```
program -> stmt*
stmt -> if_stmt | while_stmt | assign_stmt | read_stmt | write_stmt

block -> INDENT stmt+ DEDENT

if_stmt -> 'if' expr 'then' NL block ('else' NL block)?
while_stmt -> 'while' expr 'do' NL block
assign_stmt -> IDENT '=' expr NL
read_stmt -> 'read' IDENT NL
write_stmt -> 'write' expr NL

# Названия тоже из питона, извините (они хорошие!!!)
# Звездочки вместо саморекурсивных обозначений, кажется, больше раскрывают, что на самом деле происходит
expr -> or_test
or_test -> (and_test '||')* and_test
and_test -> (comp_test '&&')* comp_test
comp_test -> arith_expr | arith_expr ('==' | '/=' | '<=' | '>=' | '<' | '>') arith_expr
arith_expr -> term (('+' | '-') term)*
term -> factor (('*' | '/') factor)*
# Унарные операторы, допустимо использовать несколько подряд
factor -> ('+' | '-') factor | (atom '^')* atom
atom -> '(' expr ')' | IDENT | NAT
```
